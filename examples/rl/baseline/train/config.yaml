smarts:
  # Environment
  # sumo_gui: False # If True, enables sumo-gui display.
  # num_stack: 3 # Number of frames to stack as input to policy network.
  # agent_locator: examples.rl.proximal_policy_optimization:proximal-policy-optimization-agent-v0
  agent_locator: smarts.zoo:proximal-policy-optimization-agent-v0
  scenarios:
    - 3lane_merge_multi_agent

  # # Training
  # epochs: 5_000 # Number of training loops.

  # # Training per scenario
  # train_steps: 5_000
  # checkpoint_freq: 2_500 # Save a model every checkpoint_freq calls to env.step().
  # eval_eps: 10 # Number of evaluation epsiodes.
  # eval_freq: 2_500 # Evaluate the trained model every eval_freq steps and save the best model.


  # Args
  exp_name: PPO
    # type=str, default=os.path.basename(__file__).rstrip(".py"),
    # help="the name of this experiment")
  env_id: smarts.env:platoon-v0  
    # type=str, default="BreakoutNoFrameskip-v4",
    # help="the id of the gym environment")
  learning_rate: 2.5e-4
    # type=float, default=2.5e-4,
    # help="the learning rate of the optimizer")
  seed: 42
    # type=int, default=42,
    # help="seed of the experiment")
  total_timesteps: 10_000_000
    # type=int, default=10000000,
    # help="total timesteps of the experiments")
  torch_deterministic: true
    # type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
    # help="if toggled, `torch.backends.cudnn.deterministic=False`")
  cuda: true
    # type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
    # help="if toggled, cuda will be enabled by default")
  capture_video: true
    # type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
    # help="weather to capture videos of the agent performances (check out `videos` folder)")

  # Algorithm specific arguments
  num_envs: 1
    # type=int, default=8,
    # help="the number of parallel game environments")
  num_steps: 128
    # type=int, default=128,
    # help="the number of steps to run in each environment per policy rollout")
  anneal_lr: true
    # type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
    # help="Toggle learning rate annealing for policy and value networks")
  gae: true
    # type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
    # help="Use GAE for advantage computation")
  gamma: 0.99
    # type=float, default=0.99,
    # help="the discount factor gamma")
  gae_lambda: 0.95
    # type=float, default=0.95,
    # help="the lambda for the general advantage estimation")
  num_minibatches: 4
    # type=int, default=4,
    # help="the number of mini-batches")
  update_epochs: 4
    # type=int, default=4,
    # help="the K epochs to update the policy")
  norm_adv: true
    # type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
    # help="Toggles advantages normalization")
  clip_coef: 0.1
    # type=float, default=0.1,
    # help="the surrogate clipping coefficient")
  clip_vloss: true
    # type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
    # help="Toggles whether or not to use a clipped loss for the value function, as per the paper.")
  ent_coef: 0.01
    # type=float, default=0.01,
    # help="coefficient of the entropy")
  vf_coef: 0.5
    # type=float, default=0.5,
    # help="coefficient of the value function")
  max_grad_norm: 0.5
    # type=float, default=0.5,
    # help="the maximum norm for the gradient clipping")
  target_kl: 0.1
    # type=float, default=None,
    # help="the target KL divergence threshold")